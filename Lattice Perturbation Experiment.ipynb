{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f86fe2",
   "metadata": {},
   "source": [
    "## Lattice Perturbation Experiment\n",
    "\n",
    "This simulation studies how the Lasso regression model evolves as we continuously deform a regular lattice into a random geometric graph. Starting from a perfect square (or triangular) lattice at `perturb_scale = 0`, nodes are progressively displaced by increasing amounts, with the perturbation scale serving as a proxy for the degree of geometric disorder.\n",
    "\n",
    "The central question is how the functional form of effective resistance (in terms of distance, angle, and degree) changes across this deformation.\n",
    "\n",
    "### Scaling the radius multiplier Across Lattice Orders\n",
    "\n",
    "We noticed that when we perturb the nodes of the lattice while keeping the same radius of connection the mean degree of our graphs dropped significantly, thus we have added a correction term. The term `perturb_radius_multiplier_` is calculated as follows. Let $k_0$ denote the degree of a node and $k$ denote the epxected mean degree. We know that $k = \\mathbb{E}[k_0] = n \\pi r^2$ with $n$ the number of nodes in the graph and $r$ the radius of connection. \n",
    "\n",
    "For the order 1 square lattice i.e. only nearst vertices are connected, we have $r = \\frac{1}{\\sqrt{n}}$. Thus if we want the perturbed graph's expected mean degree to match that of the order 1 square lattice we need to scale our radius by some positive scalar $c$ so that: $$4 = n \\pi (cr)^2 \\implies \\frac{4}{\\pi} = c^2 \\implies c = \\frac{2}{\\sqrt{\\pi}} \\approx 1.128$$\n",
    "\n",
    "When the lattice is of higher order we repeat the same process, but with $r = \\frac{d}{\\sqrt{n}}$ where $d$ is ratio of the max distance between neighboring nodes and the minimum distance between neighboring nodes.\n",
    "\n",
    "We note this is an approxiamted term due to the fact that these perturbed lattices follow a different distribution than the standard RGGs where we uniformly randomly place vertices on the torus. However, this approximation works well numerically.\n",
    "\n",
    "We provide a function radius_multiplier_calc_square_lattice() to calculate the radius multiplier using the following smaller inputs:\n",
    "- Order 1: $1, 4$ \n",
    "- Order 2: $\\sqrt{2}, 8$\n",
    "- Order 3: $2, 12$\n",
    "- Order 4: $\\sqrt{5}, 20$\n",
    "- Order 5: $\\sqrt{8}, 24$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6713c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# --- IMPORTS FOR ML ---\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# --- USER PATH ---\n",
    "from RGG_Library import RGGBuilder\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# PERTURBATION MULTIPLIER HELPER FUNCTION\n",
    "# --------------------------------------------------------------------------------\n",
    "def radius_multiplier_calc_square_lattice(dist, exp_k):\n",
    "    return np.sqrt(exp_k / (dist**2 * np.pi))\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Experiment Range:\n",
    "PERTURB_VALUES = np.concatenate(([0.0], np.arange(0.1, 1.5, 0.1)))\n",
    "\n",
    "SPACE                       = [\"square_torus\", \"triangular_torus\"][0]\n",
    "USE_ANGLES                  = [True, False][0]\n",
    "LASSO_ALPHA                 = 1e-6\n",
    "CONNECTIVITY_REGIME         = [\"sc\",\"c\"][0]\n",
    "\n",
    "ORDER                       = 4\n",
    "MAX_DIST_FOR_ORDER          = np.sqrt(5)\n",
    "DEGREE_FOR_ORDER            = 20\n",
    "\n",
    "PERTURB_RADIUS_MULTIPLIER   = radius_multiplier_calc_square_lattice(dist=MAX_DIST_FOR_ORDER, exp_k=DEGREE_FOR_ORDER)\n",
    "\n",
    "N_NODES                     = 1024\n",
    "K_NEIGHBORS                 = 20\n",
    "N_SAMPLES                   = 1000\n",
    "N_BINS                      = 20\n",
    "\n",
    "N_TRAIN_GRAPHS              = 80\n",
    "N_TEST_GRAPHS               = 20\n",
    "BASE_SEED                   = 0\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# RGG BUILDER\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def build_RGG(perturb_val, iteration_seed):\n",
    "    \"\"\"\n",
    "    Generates a graph, returns samples.\n",
    "\n",
    "    NOTE ON FEATURE SHAPES (FIX 3):\n",
    "    --------------------------------\n",
    "    The feature matrix X has different numbers of columns depending on whether\n",
    "    perturbation is active:\n",
    "\n",
    "      - perturb=False (perfect lattice, p=0.0):  3 features\n",
    "        [\"Log(d)\", \"d^2\", \"d^4 * cos(4theta)\"]\n",
    "\n",
    "      - perturb=True (p > 0.0):                  4 features\n",
    "        [\"Log(d)\", \"d^2\", \"d^4 * cos(4theta)\", \"InvDegSum\"]\n",
    "\n",
    "    This is intentional: InvDegSum is only meaningful when node degrees vary\n",
    "    (i.e. under perturbation). On a perfect lattice all nodes have the same\n",
    "    degree so InvDegSum adds no signal and is excluded to keep the lattice\n",
    "    model parsimonious.\n",
    "\n",
    "    Consequence for result collection: in the simulation loop, results are\n",
    "    stored via `coefs.get(key, 0.0)`, so \"InvDegSum\" is recorded as 0.0 for\n",
    "    the unperturbed case. This is correct by design — it simply means that\n",
    "    feature is absent (zero coefficient) for the perfect-lattice model.\n",
    "\n",
    "    Since p is always the same for both train and test calls within a single\n",
    "    compute_r2 invocation, the column count will always match between train\n",
    "    and test sets, so no shape mismatch crash will occur.\n",
    "    \"\"\"\n",
    "    # --- 1. Graph Generation ---\n",
    "    if perturb_val == 0.0:\n",
    "        perturb = False\n",
    "        perturb_scale = 0.0\n",
    "    else:\n",
    "        perturb = True\n",
    "        perturb_scale = perturb_val\n",
    "    \n",
    "    builder = RGGBuilder(\n",
    "        n                           = N_NODES,\n",
    "        k                           = K_NEIGHBORS,\n",
    "        connectivity_regime         = CONNECTIVITY_REGIME,\n",
    "        space                       = SPACE,\n",
    "        order                       = ORDER,\n",
    "        perturb                     = perturb,\n",
    "        perturb_scale               = perturb_scale,\n",
    "        perturb_radius_multiplier   = PERTURB_RADIUS_MULTIPLIER,\n",
    "        seed                        = iteration_seed\n",
    "    )\n",
    "    \n",
    "    G = builder.build()\n",
    "\n",
    "    # Ensure Giant Component\n",
    "    if not nx.is_connected(G):\n",
    "        components = list(nx.connected_components(G))\n",
    "        Gsub = G.subgraph(max(components, key=len)).copy()\n",
    "    else:\n",
    "        Gsub = G\n",
    "    \n",
    "    Gsub = nx.convert_node_labels_to_integers(Gsub, ordering=\"sorted\")\n",
    "\n",
    "    # Calculate Mean Degree: (2 * edges) / nodes\n",
    "    mean_degree = 2 * Gsub.number_of_edges() / Gsub.number_of_nodes()\n",
    "\n",
    "    # --- 2. Sample Data ---\n",
    "    random.seed(iteration_seed)\n",
    "    np.random.seed(iteration_seed)\n",
    "\n",
    "    # Correcting the minimum distance for sampling if perturbation occurs\n",
    "    if perturb == False:\n",
    "        min_distances = builder.radius\n",
    "    else:\n",
    "        min_distances = builder.radius * PERTURB_RADIUS_MULTIPLIER\n",
    "    \n",
    "    if USE_ANGLES:\n",
    "        res, degs, dists, pairs, angles = RGGBuilder.sample_commute_times_even_distance_w_angles(\n",
    "            Gsub, nsamples=N_SAMPLES, n_bins=N_BINS, seed=iteration_seed, \n",
    "            min_dist=min_distances, max_dist=2\n",
    "        )\n",
    "    else:\n",
    "        res, degs, dists, pairs = RGGBuilder.sample_commute_times_even_distance(\n",
    "            Gsub, nsamples=N_SAMPLES, n_bins=N_BINS, seed=iteration_seed, \n",
    "            min_dist=min_distances, max_dist=2\n",
    "        )\n",
    "        angles = np.zeros_like(dists)\n",
    "\n",
    "    # --- 3. Feature Engineering ---\n",
    "    safe_dists = dists.copy()\n",
    "    safe_dists[safe_dists == 0] = 1e-9 \n",
    "\n",
    "    # See docstring above for explanation of why feature sets differ.\n",
    "    if perturb == False:\n",
    "        feature_dict = {\n",
    "            \"Log(d)\": np.log(safe_dists),\n",
    "            \"d^2\": dists**2,\n",
    "            \"d^4 * cos(4theta)\": dists**4 * np.cos(4*angles),\n",
    "        }\n",
    "    else:\n",
    "        feature_dict = {\n",
    "            \"Log(d)\": np.log(safe_dists),\n",
    "            \"d^2\": dists**2,\n",
    "            \"d^4 * cos(4theta)\": dists**4 * np.cos(4*angles),\n",
    "            \"InvDegSum\": degs,\n",
    "        }\n",
    "\n",
    "    feature_names = list(feature_dict.keys())\n",
    "    X = np.column_stack(list(feature_dict.values()))\n",
    "    y = res\n",
    "\n",
    "    return X, y, feature_names, mean_degree\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# COMPUTING R^2\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def compute_r2(n, k, p, base_seed=BASE_SEED):\n",
    "    \"\"\"\n",
    "    Train LASSO on N_TRAIN_GRAPHS pooled graphs, test on N_TEST_GRAPHS pooled graphs.\n",
    "    Returns a dict of coefficients, metadata, and out-of-sample R². Returns NaN on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- TRAINING: pool data from N_TRAIN_GRAPHS graphs ---\n",
    "        X_train_parts, y_train_parts = [], []\n",
    "        mean_degrees = []  # FIX 4: collect mean_degree from all training graphs\n",
    "\n",
    "        for i in range(N_TRAIN_GRAPHS):\n",
    "            X, y, feature_names, mean_degree = build_RGG(p, base_seed + i)\n",
    "            if X is not None:\n",
    "                X_train_parts.append(X)\n",
    "                y_train_parts.append(y)\n",
    "                mean_degrees.append(mean_degree)  # FIX 4: accumulate\n",
    "\n",
    "        if len(X_train_parts) == 0:\n",
    "            return np.nan\n",
    "\n",
    "        X_train = np.vstack(X_train_parts)\n",
    "        y_train = np.concatenate(y_train_parts)\n",
    "\n",
    "        # FIX 4: average mean_degree across all training graphs rather than\n",
    "        # using only the last graph's value (graphs may differ slightly in size\n",
    "        # after giant-component extraction)\n",
    "        mean_degree_avg = np.mean(mean_degrees)\n",
    "\n",
    "        # --- FIT LASSO ---\n",
    "        # FIX 2: use LASSO_ALPHA config variable instead of hardcoded 1e-6\n",
    "        model = make_pipeline(\n",
    "            StandardScaler(),\n",
    "            Lasso(alpha=LASSO_ALPHA, random_state=0, max_iter=50000)\n",
    "        )\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "        lasso  = model.named_steps['lasso']\n",
    "        scaler = model.named_steps['standardscaler']\n",
    "        real_coefs     = lasso.coef_ / scaler.scale_\n",
    "        real_intercept = lasso.intercept_ - np.sum(real_coefs * scaler.mean_)\n",
    "\n",
    "        # --- TESTING: pool data from N_TEST_GRAPHS graphs ---\n",
    "        # Use seeds that don't overlap with training seeds\n",
    "        X_test_parts, y_test_parts = [], []\n",
    "        for i in range(N_TEST_GRAPHS):\n",
    "            X, y, _, _ = build_RGG(p, base_seed + N_TRAIN_GRAPHS + i)\n",
    "            if X is not None:\n",
    "                X_test_parts.append(X)\n",
    "                y_test_parts.append(y)\n",
    "\n",
    "        # FIX 1: guard is now OUTSIDE the loop at the correct indentation level\n",
    "        # (previously it was inside 'if X is not None', so it could never trigger)\n",
    "        if len(X_test_parts) == 0:\n",
    "            return np.nan\n",
    "\n",
    "        X_test = np.vstack(X_test_parts)\n",
    "        y_test = np.concatenate(y_test_parts)\n",
    "\n",
    "        # --- OUT-OF-SAMPLE R² ---\n",
    "        y_pred  = X_test @ real_coefs + real_intercept\n",
    "        ss_res  = np.sum((y_test - y_pred) ** 2)\n",
    "        ss_tot  = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "        r2      = 1 - ss_res / ss_tot\n",
    "\n",
    "        # Build result dictionary\n",
    "        results = dict(zip(feature_names, real_coefs))\n",
    "        results[\"Intercept\"]   = real_intercept\n",
    "        results[\"Mean Degree\"] = mean_degree_avg  # FIX 4: averaged value\n",
    "        results[\"R^2\"]         = r2\n",
    "        \n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [WARN] n={n}, k={k} failed: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# SIMULATION\n",
    "# --------------------------------------------------------------------------------\n",
    "history = {key: [] for key in [\n",
    "        \"R^2\",\n",
    "        \"Intercept\",\n",
    "        \"Mean Degree\",\n",
    "        \"Log(d)\", \n",
    "        \"d^2\", \n",
    "        \"d^4 * cos(4theta)\", \n",
    "        \"InvDegSum\",\n",
    "    ]}\n",
    "\n",
    "print(f\"Starting Geometric Experiment: {len(PERTURB_VALUES)} steps.\")\n",
    "print(\"Step 1: Perturb=False (Perfect Lattice)\")\n",
    "print(\"Steps 2+: Perturb=True\")\n",
    "\n",
    "perturb_vals_plotted = []\n",
    "\n",
    "for i, p in enumerate(tqdm(PERTURB_VALUES)):\n",
    "    try:\n",
    "        coefs = compute_r2(N_NODES, K_NEIGHBORS, p, base_seed=BASE_SEED)\n",
    "            \n",
    "        for key in history.keys():\n",
    "            history[key].append(coefs.get(key, 0.0))\n",
    "            \n",
    "        perturb_vals_plotted.append(p)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping p={p:.2f} due to error: {e}\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# VISUALIZATION\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Layout: 2 (top) - 3 (middle) - 2 (bottom)\n",
    "# Top row:    Intercept, InvDegSum         (centred)\n",
    "# Middle row: Log(d), d^2, d^4*cos(4theta)\n",
    "# Bottom row: R^2, Mean Degree             (centred)\n",
    "\n",
    "TOP_ROW    = [\"Intercept\", \"InvDegSum\"]\n",
    "MIDDLE_ROW = [\"Log(d)\", \"d^2\", \"d^4 * cos(4theta)\"]\n",
    "BOTTOM_ROW = [\"R^2\", \"Mean Degree\"]\n",
    "\n",
    "def get_color(feature_name):\n",
    "    if feature_name == \"R^2\":          return 'black'\n",
    "    elif \"Log\" in feature_name:        return 'tab:green'\n",
    "    elif \"1/\" in feature_name:         return 'tab:red'\n",
    "    elif \"^2\" in feature_name:         return 'tab:orange'\n",
    "    elif \"Degree\" in feature_name:     return 'purple'\n",
    "    elif \"Intercept\" in feature_name:  return 'tab:cyan'\n",
    "    elif \"InvDeg\" in feature_name:     return 'tab:red'\n",
    "    else:                              return 'tab:blue'\n",
    "\n",
    "def plot_ax(ax, feature_name):\n",
    "    y_vals = history[feature_name]\n",
    "    color  = get_color(feature_name)\n",
    "\n",
    "    ax.plot(perturb_vals_plotted, y_vals,\n",
    "            marker='o', markersize=3, linewidth=1.5, color=color, alpha=0.8)\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "    if feature_name in (\"R^2\", \"Mean Degree\"):\n",
    "        y_min, y_max = min(y_vals), max(y_vals)\n",
    "        padding = (y_max - y_min) * 0.25 if y_max != y_min else 0.5\n",
    "        ax.set_ylim(y_min - padding, y_max + padding)\n",
    "\n",
    "    ax.set_title(feature_name, fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel(\"Perturbation Scale\")\n",
    "    ax.set_ylabel(\"Coefficient Value\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Build figure with GridSpec ---\n",
    "# Use a 6-column grid so that:\n",
    "#   - 2-panel rows occupy cols [1:3] and [3:5]  → centred over 6 cols\n",
    "#   - 3-panel rows occupy cols [0:2], [2:4], [4:6]\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs  = fig.add_gridspec(3, 6)\n",
    "\n",
    "# Top row: 2 panels centred (start at col 1, each spans 2 cols)\n",
    "top_axes = [\n",
    "    fig.add_subplot(gs[0, 1:3]),\n",
    "    fig.add_subplot(gs[0, 3:5]),\n",
    "]\n",
    "\n",
    "# Middle row: 3 panels spanning all 6 cols (each spans 2 cols)\n",
    "mid_axes = [\n",
    "    fig.add_subplot(gs[1, 0:2]),\n",
    "    fig.add_subplot(gs[1, 2:4]),\n",
    "    fig.add_subplot(gs[1, 4:6]),\n",
    "]\n",
    "\n",
    "# Bottom row: 2 panels centred (same as top)\n",
    "bot_axes = [\n",
    "    fig.add_subplot(gs[2, 1:3]),\n",
    "    fig.add_subplot(gs[2, 3:5]),\n",
    "]\n",
    "\n",
    "for ax, name in zip(top_axes, TOP_ROW):\n",
    "    plot_ax(ax, name)\n",
    "\n",
    "for ax, name in zip(mid_axes, MIDDLE_ROW):\n",
    "    plot_ax(ax, name)\n",
    "\n",
    "for ax, name in zip(bot_axes, BOTTOM_ROW):\n",
    "    plot_ax(ax, name)\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Evolution of Geometric Coefficients (p=0 is Perfect Lattice)\\n(Lasso Alpha={LASSO_ALPHA})\",\n",
    "    fontsize=16\n",
    ")\n",
    "\n",
    "fig.subplots_adjust(top=0.90, hspace=0.5, wspace=0.4)\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# SAVE RESULTS TO DATAFRAME\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Build DataFrame from history dict.\n",
    "# Each row = one perturbation value; each column = one tracked quantity.\n",
    "# NOTE: InvDegSum will be 0.0 for the unperturbed (p=0) row — this is expected,\n",
    "# as that feature is excluded from the lattice model (see build_RGG docstring).\n",
    "df_results = pd.DataFrame(history, index=perturb_vals_plotted)\n",
    "df_results.index.name = \"Perturbation Scale\"\n",
    "\n",
    "# Add a convenience column flagging whether a row used a perturbed graph\n",
    "df_results[\"Perturbed\"] = df_results.index > 0.0\n",
    "\n",
    "# Reorder columns: metadata first, then coefficients\n",
    "META_COLS    = [\"R^2\", \"Mean Degree\", \"Perturbed\"]\n",
    "COEF_COLS    = [\"Intercept\", \"Log(d)\", \"d^2\", \"d^4 * cos(4theta)\", \"InvDegSum\"]\n",
    "df_results   = df_results[META_COLS + COEF_COLS]\n",
    "\n",
    "print(\"\\n--- Results DataFrame ---\")\n",
    "print(df_results.to_string())\n",
    "\n",
    "# --- Save to CSV ---\n",
    "CSV_PATH = \"Latice_Perturbation_Experiment_Results.csv\"\n",
    "df_results.to_csv(CSV_PATH)\n",
    "print(f\"\\nDataFrame saved to {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8280c7d9",
   "metadata": {},
   "source": [
    "## Older Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ba567",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# --- IMPORTS FOR ML ---\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# --- USER PATH ---\n",
    "from RGG_Library import RGGBuilder\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# PERTURBATION MULTIPLIER HELPER FUNCTION\n",
    "# --------------------------------------------------------------------------------\n",
    "def radius_multiplier_calc_square_lattice(dist, exp_k):\n",
    "    return np.sqrt(exp_k / (dist**2 * np.pi))\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Experiment Range:\n",
    "PERTURB_VALUES = np.concatenate(([0.0], np.arange(0.5, 10.5, 0.5)))\n",
    "\n",
    "SPACE                       = [\"square_torus\", \"triangular_torus\"][0]\n",
    "USE_ANGLES                  = [True, False][0]\n",
    "LASSO_ALPHA                 = 1e-6\n",
    "CONNECTIVITY_REGIME         = [\"sc\",\"c\"][0]\n",
    "\n",
    "ORDER                       = 4\n",
    "MAX_DIST_FOR_ORDER          = np.sqrt(5)\n",
    "DEGREE_FOR_ORDER            = 20\n",
    "\n",
    "PERTURB_RADIUS_MULTIPLIER   = radius_multiplier_calc_square_lattice(dist=MAX_DIST_FOR_ORDER, exp_k=DEGREE_FOR_ORDER)\n",
    "\n",
    "N_NODES                     = 256\n",
    "K_NEIGHBORS                 = 20\n",
    "N_SAMPLES                   = 1000\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# SIMULATION FUNCTION\n",
    "# --------------------------------------------------------------------------------\n",
    "def get_coefficients_for_perturbation(perturb_val, iteration_seed):\n",
    "    \"\"\"\n",
    "    Generates a graph, runs Lasso, and returns Real Coefs + Intercept + Mean Degree.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Graph Generation ---\n",
    "    if perturb_val == 0.0:\n",
    "        perturb = False\n",
    "        perturb_scale = 0.0\n",
    "    else:\n",
    "        perturb = True\n",
    "        perturb_scale = perturb_val\n",
    "    \n",
    "    builder = RGGBuilder(\n",
    "        n=N_NODES,\n",
    "        k=K_NEIGHBORS,\n",
    "        connectivity_regime=CONNECTIVITY_REGIME,\n",
    "        space=SPACE,\n",
    "        order=ORDER,\n",
    "        perturb=perturb,\n",
    "        perturb_scale=perturb_scale,\n",
    "        perturb_radius_multiplier=PERTURB_RADIUS_MULTIPLIER,\n",
    "        seed=iteration_seed\n",
    "    )\n",
    "    \n",
    "    G = builder.build()\n",
    "\n",
    "    # Ensure Giant Component\n",
    "    if not nx.is_connected(G):\n",
    "        components = list(nx.connected_components(G))\n",
    "        Gsub = G.subgraph(max(components, key=len)).copy()\n",
    "    else:\n",
    "        Gsub = G\n",
    "    \n",
    "    Gsub = nx.convert_node_labels_to_integers(Gsub, ordering=\"sorted\")\n",
    "\n",
    "    # --- NEW: Calculate Mean Degree ---\n",
    "    # (2 * edges) / nodes\n",
    "    mean_degree = 2 * Gsub.number_of_edges() / Gsub.number_of_nodes()\n",
    "\n",
    "    # --- 2. Sample Data ---\n",
    "    random.seed(iteration_seed)\n",
    "    np.random.seed(iteration_seed)\n",
    "    \n",
    "    if USE_ANGLES:\n",
    "        res, degs, dists, pairs, angles = RGGBuilder.sample_commute_times_even_distance_w_angles(\n",
    "            Gsub, nsamples=N_SAMPLES, n_bins=20, seed=iteration_seed, \n",
    "            min_dist=builder.radius, max_dist=2\n",
    "        )\n",
    "    else:\n",
    "        res, degs, dists, pairs = RGGBuilder.sample_commute_times_even_distance(\n",
    "            Gsub, nsamples=N_SAMPLES, n_bins=20, seed=iteration_seed, \n",
    "            min_dist=builder.radius, max_dist=2\n",
    "        )\n",
    "        angles = np.zeros_like(dists)\n",
    "\n",
    "    # --- 3. Feature Engineering ---\n",
    "    safe_dists = dists.copy()\n",
    "    safe_dists[safe_dists == 0] = 1e-9 \n",
    "\n",
    "    # Define features based on perturbation state\n",
    "    if perturb == False:\n",
    "        feature_dict = {\n",
    "            \"Log(d)\": np.log(safe_dists),\n",
    "            \"d^2\": dists**2,\n",
    "            \"d^4 * cos(4theta)\": dists**4 * np.cos(4*angles),\n",
    "        }\n",
    "    else:\n",
    "        feature_dict = {\n",
    "            \"Log(d)\": np.log(safe_dists),\n",
    "            \"d^2\": dists**2,\n",
    "            \"d^4 * cos(4theta)\": dists**4 * np.cos(4*angles),\n",
    "            \"InvDegSum\": degs,\n",
    "        }\n",
    "\n",
    "    feature_names = list(feature_dict.keys())\n",
    "    X = np.column_stack(list(feature_dict.values()))\n",
    "    y = res\n",
    "\n",
    "    # --- 4. Lasso Regression ---\n",
    "    model = make_pipeline(\n",
    "        StandardScaler(), \n",
    "        Lasso(alpha=LASSO_ALPHA, max_iter=50000, tol=1e-4)\n",
    "    )\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        model.fit(X, y)\n",
    "        # --- NEW: Calculate R^2 Score ---\n",
    "        r2_score = model.score(X, y)\n",
    "\n",
    "    # --- 5. Extract Unscaled Coefficients & Intercept ---\n",
    "    lasso = model.named_steps['lasso']\n",
    "    scaler = model.named_steps['standardscaler']\n",
    "    \n",
    "    sigmas = scaler.scale_\n",
    "    sigmas[sigmas == 0] = 1.0 \n",
    "    \n",
    "    real_coefs = lasso.coef_ / sigmas\n",
    "    real_intercept = lasso.intercept_ - np.dot(real_coefs, scaler.mean_)\n",
    "\n",
    "    # Create result dictionary\n",
    "    results = dict(zip(feature_names, real_coefs))\n",
    "    \n",
    "    # Add metadata\n",
    "    results[\"Intercept\"] = real_intercept\n",
    "    results[\"Mean Degree\"] = mean_degree\n",
    "    results[\"R^2\"] = r2_score  # <--- Added\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# MAIN EXECUTION LOOP\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "history = {key: [] for key in [\n",
    "    \"R^2\",\n",
    "    \"Intercept\",\n",
    "    \"Mean Degree\",\n",
    "    \"Log(d)\", \n",
    "    \"d^2\", \n",
    "    \"d^4 * cos(4theta)\", \n",
    "    \"InvDegSum\",\n",
    "]}\n",
    "\n",
    "print(f\"Starting Geometric Experiment: {len(PERTURB_VALUES )} steps.\")\n",
    "print(\"Step 1: Perturb=False (Perfect Lattice)\")\n",
    "print(\"Steps 2+: Perturb=True (Scale epsilon -> 1.5)\")\n",
    "\n",
    "perturb_vals_plotted = []\n",
    "\n",
    "for i, p in enumerate(tqdm(PERTURB_VALUES)):\n",
    "    try:\n",
    "        coefs = get_coefficients_for_perturbation(p, iteration_seed=0)\n",
    "        \n",
    "        for key in history.keys():\n",
    "            history[key].append(coefs.get(key, 0.0))\n",
    "        \n",
    "        perturb_vals_plotted.append(p)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping p={p:.2f} due to error: {e}\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# VISUALIZATION\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "features_to_plot = list(history.keys())\n",
    "n_feats = len(features_to_plot)\n",
    "cols = 3\n",
    "rows = (n_feats // cols) + (1 if n_feats % cols > 0 else 0)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(14, 3.5 * rows), constrained_layout=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature_name in enumerate(features_to_plot):\n",
    "    ax = axes[i]\n",
    "    y_vals = history[feature_name]\n",
    "    \n",
    "    if feature_name == \"R^2\":\n",
    "        color = 'black'\n",
    "        ax.set_ylim(0, 1.05)\n",
    "    elif \"Log\" in feature_name: color = 'tab:green'\n",
    "    elif \"1/\" in feature_name: color = 'tab:red'\n",
    "    elif \"^2\" in feature_name: color = 'tab:orange'\n",
    "    elif \"Degree\" in feature_name: color = 'purple'\n",
    "    else: color = 'tab:blue'\n",
    "    \n",
    "    ax.plot(perturb_vals_plotted, y_vals, marker='o', markersize=3, linewidth=1.5, color=color, alpha=0.8)\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    ax.set_title(feature_name, fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel(\"Perturbation Scale\")\n",
    "    ax.set_ylabel(\"Coefficient Value\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Evolution of Geometric Coefficients (p=0 is Perfect Lattice)\\n(Lasso Alpha={LASSO_ALPHA})\", fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
